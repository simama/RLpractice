{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dagger RL Simple Implementation  \n",
    "Dagger is a reinforcement learning algorithm for imitation learning/behaviour cloning. Introdiced in paper https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf  \n",
    "It uses initial expert knowledge (usually human labeled data) to perform surprevised learning of agent's policy (mapping from observations to actions). Main trick in Dagger is that after agent learns initial policy (from expert data), it uses that policy to act in real environemnt and stores those experiences (observations). These real observations are then passed to expert to be labeled (add expert's actions), and are added to the training set. Then agent policy is trained again, this time on the new augmented data set, and cycle is repeated.        \n",
    "  \n",
    "Main trick in Dagger is dataset augmentation from agent's own experince. Inital expert dataset is limited and it is very likely that agent will diverge from the expert's path and encounter new states. Initial policy is almost useless in those new situations. By obtaining expert labels for those new observations and retraining the policy, agent becomes more robust to path perturbations.  \n",
    "  \n",
    "This implementation is made for UC Berkely course CS 294 Deep Reinforcement Learning. It is a naive implementation (still unfinished curentlly) by extending previous ordinary imitation learning technique, uses provided expert policy for gathering expert's dataset, and acts in MuJoCo environment.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(train_data):\n",
    "    \"\"\"Data preprocessing - mean substraction and normalization\"\"\"\n",
    "    \n",
    "    data_mean = np.mean(train_data['observations'], axis = 0)\n",
    "    train_data['observations'] -= data_mean\n",
    "    input_dim = train_data['observations'].shape[1]\n",
    "    action_dim = train_data['actions'].shape[2]\n",
    "    data_combined = zip(train_data['observations'], train_data['actions'])\n",
    "    return data_mean, train_data, input_dim, action_dim, data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Netowrk definition \n",
    "\n",
    "class Network():\n",
    "    def __init__(self, input_dim, action_dim, hidden1_units, hidden2_units, regularization = False, beta = 0.01):\n",
    "        \"\"\"Network definition\"\"\"\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.h1_num = hidden1_units\n",
    "        self.h2_num = hidden2_units\n",
    "        self.input_observations = tf.placeholder(tf.float32, shape=(None,self.input_dim))\n",
    "        self.action_labels = tf.placeholder(tf.float32, shape=(None,self.action_dim))\n",
    "\n",
    "        self.w1 = tf.Variable(tf.truncated_normal([self.input_dim, self.h1_num],\n",
    "                                                  stddev=1.0 / math.sqrt(float(self.input_dim))),name='w1')\n",
    "        self.b1 = tf.Variable(tf.zeros(self.h1_num),name='b1')\n",
    "        self.h1 = tf.nn.relu(tf.matmul(self.input_observations,self.w1) + self.b1)\n",
    "\n",
    "        self.w2 = tf.Variable(tf.truncated_normal([self.h1_num, self.h2_num],\n",
    "                                             stddev=1.0 / math.sqrt(float(self.h1_num))),name='w2')\n",
    "        self.b2 = tf.Variable(tf.zeros(self.h2_num),name='b2')\n",
    "        self.h2 = tf.nn.relu(tf.matmul(self.h1,self.w2) + self.b2)\n",
    "\n",
    "        self.w3 = tf.Variable(tf.truncated_normal([self.h2_num, self.action_dim],\n",
    "                                             stddev=1.0 / math.sqrt(float(self.h2_num))),name='w3')\n",
    "        self.b3 = tf.Variable(tf.zeros(self.action_dim),name='b3')\n",
    "        self.output = tf.matmul(self.h2,self.w3) + self.b3\n",
    "\n",
    "        self.error = tf.reduce_mean(tf.pow(tf.subtract(self.output,self.action_labels),2))\n",
    "        if regularization:\n",
    "            self.regularizers = tf.nn.l2_loss(self.w1) + tf.nn.l2_loss(self.w2) + tf.nn.l2_loss(self.w3)\n",
    "            self.error = self.error + beta * self.regularizers\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.error)\n",
    "\n",
    "    def train(self, sess, saver, train_data, training_epochs, batch_size):\n",
    "        \"\"\"Supervised training of agent network\"\"\"\n",
    "        \n",
    "        total_batch = int(len(train_data)/batch_size)\n",
    "        for epoch in xrange(training_epochs):\n",
    "            batch_count = 0\n",
    "            avg_cost = 0.\n",
    "      \n",
    "            # Loop over all batches\n",
    "            for i in xrange(total_batch):\n",
    "                next_batch = random.sample(train_data, batch_size)\n",
    "                next_batch = zip(*next_batch)\n",
    "                batch_x = next_batch[0]\n",
    "                batch_y = np.asarray(next_batch[1])\n",
    "                batch_y = batch_y.reshape((batch_size,self.action_dim))\n",
    "\n",
    "                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                _, c = sess.run([self.optimizer, self.error], \n",
    "                                feed_dict={self.input_observations: batch_x, self.action_labels: batch_y})\n",
    "                \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "                if i % 10000 == 0:\n",
    "                    print(\"Batch number {:d}\".format(i))\n",
    "                    #print(\"Step {} | Average cost {}\".format(i, avg_cost))\n",
    "            \n",
    "            # Display logs per epoch step\n",
    "            print(\"Epoch: {:04d}, cost = {:.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "        print \"Optimization Finished!\"\n",
    "        saver.save(sess, path + '/' + environment + '.cptk')\n",
    "        print (\"Model Saved\")\n",
    "        \n",
    "    def run(self, sess, saver, env, num_rollouts, render = False, load_model = True):\n",
    "        \"\"\"Run policy on real observations\"\"\"\n",
    "       \n",
    "        returns = []\n",
    "        observations = []\n",
    "        max_step = env.spec.timestep_limit\n",
    "        \n",
    "        if load_model:\n",
    "            print('Loading Model...')\n",
    "            ckpt = tf.train.get_checkpoint_state(path)\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "        else: \n",
    "            sess.close()\n",
    "\n",
    "        for i in range(num_rollouts):\n",
    "            print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                observations.append(obs)\n",
    "                obs = obs.reshape((1,self.input_dim))\n",
    "                obs -= data_mean\n",
    "                action = sess.run(self.output, feed_dict={self.input_observations: obs})\n",
    "                obs, r, done, _ = env.step(action)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if steps >= max_step:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "        \n",
    "        self.agent_data = {'observations': np.array(observations)}\n",
    "        with open(\"Hopper-v1\" + '_agent_data.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.agent_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"Agent data pickled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(num_cycles, environment):\n",
    "    with open(\"Hopper-v1\" + '_expert_data.pickle', 'rb') as handle:\n",
    "        train_data = pickle.load(handle)\n",
    "    data_mean, train_data, input_dim, action_dim, data_combined = data_preprocessing(train_data)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        agent = Network(input_dim,action_dim, hidden1_units, hidden2_units)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for cycle in xrange(num_cycles):\n",
    "            agent.train(sess, saver, data_combined, training_epochs, batch_size)\n",
    "            agent.run(sess, saver, environment)\n",
    "            break\n",
    "            expert_data = expert_run(agent_observations)\n",
    "            train_data.append(expert_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "num_rollouts = 20\n",
    "beta = 0.001\n",
    "path = './dagger_policy'\n",
    "\n",
    "# Network Parameters\n",
    "hidden1_units = 128 # 1st layer number of features\n",
    "hidden2_units = 128 # 2nd layer number of features\n",
    "\n",
    "environments = {1: \"Ant-v1\", 2: \"HalfCheetah-v1\", 3: \"Hopper-v1\", \n",
    "                4: \"Humanoid-v1\", 5: \"Reacher-v1\", 6: \"Walker2d-v1\"}\n",
    "environment = environments[3]\n",
    "env = gym.make(environment)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "main(1,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
